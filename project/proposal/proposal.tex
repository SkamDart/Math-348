\documentclass[a4paper]{article}

\usepackage{amsmath}
\newenvironment{definition}[1][Formal Definition]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{indef}[1][Informal Definition]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}} 

\title{Machine Learning Algorithms Illustrated in Python}
\author{Cameron P. Dart}

\begin{document}
\maketitle
% Abstract
\begin{abstract}
In order to gain a deeper understanding of artificial intelligence algorithms, I write on commonly used \textit{supervised} and \textit{unsupervised} learning algorithms. While researching these algorithms, I will create my own implementations of each in Python, describe and prove the mathematical reasoning, and analyze the space-time complexities of my own to open source implementations of these same algorithms in Big $\mathcal{O}$, $\theta$, and $\Omega$ notation.
\end{abstract}

% Intro
\section{Introduction}
In this paper, I will strictly define machine learning and give mathematical descriptions and intuition on one \textit{supervised} and one \textit{unsupervised} learning algorithm. Additionally, I will provide an explanation of the differences between $\mathcal{O}(n)$,$\Theta(n)$, and $\Omega(n)$ in regards run time and space complexity. Lastly, I will combine the mathematical descriptions of algorithms and space/time complexity to analyze the differences between my own and open source implementations of the same algorithms.Machine Learning algorithms are repetitive in their computations so before writing any code, it would be most useful to write the algorithm by hand with a small sample set. This will be beneficial to understanding where inefficiencies in code could occur, or what mathematical concepts I can exploit using the computational power of a computer. Once I have the code written, analysis, I will move forward to writing my own algorithm in Python.   

\section{Method of Analysis}
In order to get a better understanding of the algorithms used, I will use Python modules such as \textit{memory} and \textit{os} \cite{WikiPython} to gather data on  programs, functions, and processes run time and memory usage. The module \textit{memory} will be used as a way to keep track of how much memory a Python process is consuming. While \textit{os} will be used to test the runtime a certain function. 
I will provide line by line theoretical analysis with run times of functions and operations provided by Python documentation so I can further see the short comings and advantages of each implementation. Additionally, I will prove which runs more quickly and uses less space through test cases. I will run test cases with controlled input size and examine the results to confirm my hypothesizes. With all of the theoretical and experimental data at my hand, I will be able to decide which implementation the most efficient.  

% Big O Analysis 
\section{Space-Complexity Analysis}
Let there exist positive constants $c_o$, $c_1$, $c_2$, and  $n_0$.\\ As well as functions $f$ and $g$
\subsection{Big $\mathcal{O}(n)$} 
\begin{definition} 
\leavevmode
$f(n) \leq Cg(n)$ whenever $n>n_0$\\
$f(n)$ is $ \mathcal{O}(g(n)) \equiv \; (\exists C) \; (\exists n_0) \; (\forall n) \;(n > n_0 \rightarrow f(n) \leq Cg(n))$
\cite{Auckland}
\end{definition}
\begin{indef}
A function $f$ is considered to be $\mathcal(O){g}$ if it is never larger than some constant multiplied by $g$.
\end{indef}

% big omega
\subsection{Big $\Omega(n)$}
\begin{definition}Let $C$ and $n_0$ be positive constants  \\
$f(n)$ is $ \Omega(g(n)) \equiv \; (\exists C) \; (\exists n_0) \; (\forall n) \;(n > n_0 \rightarrow f(n) \geq Cg(n))$
\cite{Auckland}
\end{definition}
\begin{indef}
A function $f$ is considered to be $\Omega(n)$ if it is always larger than some constant multiplied by $g$	
\end{indef}

% big theta
\subsection{Big $\Theta(n)$}
\begin{definition}
the function $f$ is considered to be $\Theta(g(n)) \iff (c_1g(n)) \leq f(n) \leq (c_2g(n))$
\cite{Auckland} 
\end{definition}
\begin{indef}
A function $f$ is considered to be $\Theta(g(n))$ if it is bounded by two separate functions.	
\end{indef}

% Machine Learning
\section{Machine Learning}
The field of study that gives computers the ability to learn without being explicitly programmed. In general, Machine Learning algorithms produce patterns based on given input and use the patterns to predict future cases. 

% Supervised Learning
\section{Supervised Learning Algorithms}
\subsection{Definition}
\textit{Supervised Learning} is the task of deducing a function from a given\textit{training set}. \cite{Ng} A \textit{training set} is a vector used for the initial discovery of relationships between variables; ie to fit the weights of the classifying function. In order to prevent \textit{overfitting}, deducing conclusions when none exist, a \textit{validation set} is used in case any classification parameter needs to be adjusted. Further on, a \textit{test set} is used to gauge the efficiency of a given model. One classic example of a \textit{supervised learning} algorithm is a \textit{linear regression}.
\subsection{Example: Linear Regression}
\subsubsection{Definition}
A \textit{linear regression} is an approach for modeling the relationship between a scalar dependent variable $y$ and one or more explanatory variables (or independent variables) denoted $X$. Specifically the case of one explanatory variable is called a \textit{linear regression}. A sample data set for a linear regression would be pairs $(x_1,y_1),(x_2,y_2),....,(x_n,y_n)$ where the regression models $y_i$ as a function of $x_i$
\subsubsection{Mathematical Description}
The \textit{regression line} can be determined by finding \textit{sum of squares} method. A
\textit{fitted linear regression line} can be described as
\begin{equation}
\hat y=\hat \beta_{0}+\hat \beta_1*x \, \, \, \, \, \,\cite{GATech}
\end{equation}
$\hat \beta_0$ is defined as 
\begin{equation}
	\hat \beta_0 = \bar y - \hat \beta_1 \bar x
\end{equation}
Given $n$ is the number of data points in the \textit{training set}
\begin{equation}
	\hat \beta_1 = \left(\frac{n \sum x_iy_i - (\sum x_i)(\sum y_i)}{n\sum x_i^2 - (\sum x_i)^2}\right)
\end{equation}
An easier way to express $\hat \beta_1$ can be defined as
\begin{equation}
	\hat \beta_1 = \frac{S_{xy}}{S_{xx}}
\end{equation}
Where $S_{xy}$ and $S_{xx}$ are 
\begin{align*}
S_{xy}&= \sum(x_i-\bar x)(y_i-\bar y) \\
	  &= \sum x_iy_i-\frac{(\sum x_i)(\sum y_i)}{n}
\end{align*}

\begin{align*}
S_{xx} &= \sum(x_i - \bar x)^2 \\
	 &= \sum (x_i)^2 - \frac{(\sum x_i)^2}{n}
\end{align*}
Given $\bar y \equiv \frac{1}{n} \sum y_i$ and $\bar x \equiv \frac{1}{n}\sum x_i$

\subsubsection{Example Use Cases}
A doctor is given  
You are given a vector $V$  of inputs such that $V = \left \{(x_1,y_1), ... ,(x_n,y_n) \right \}$ where $x_i$ is the height and $y_i$ is an weight for the $i^{it}$ individual. \\
You want to predict the weight of an individual
%\section{Unsupervised Learning Algorithms}
%\subsection{Definition}
%\subsection{Example K-Means Clustering}
%\subsubsection{Defintion}
%\subsubsection{Mathematical Description}

\begin{thebibliography}{9}
\bibitem{Auckland}https://www.cs.auckland.ac.nz/courses/compsci220s1t/lectures/lecturenotes/GG-lectures/220handout-lecture03.pdf
\bibitem{WikiPython}https://wiki.python.org/moin/TimeComplexity
\bibitem{GATech}http://www2.isye.gatech.edu/~sman/courses/6739/SimpleLinearRegression.pdf
\bibitem{Ng}http://cs229.stanford.edu/notes/cs229-notes1.pdf
\end{thebibliography}

\end{document}